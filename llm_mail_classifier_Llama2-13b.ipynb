{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMn6fHXMzT4/ntUErAomzQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-mueller92/llm_mail_classifier/blob/main/llm_mail_classifier_Llama2-13b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYNb_DIGPm0d",
        "outputId": "da19af50-2149-4764-b249-f4e81d6a9a5b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.23)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "LkRGzankQ8Vw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modelpath=hf_hub_download(repo_id='TheBloke/em_german_mistral_v01-GGUF', filename=\"em_german_mistral_v01.Q8_0.gguf\")\n",
        "\n",
        "modelpath2=hf_hub_download(repo_id='TheBloke/Llama-2-13B-chat-GGUF', filename=\"llama-2-13b-chat.Q5_K_M.gguf\")\n"
      ],
      "metadata": {
        "id": "yUMldzTCRBLf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "      model_path=modelpath2,\n",
        "      n_ctx=4000,\n",
        "      n_batch=1024,\n",
        "      n_gpu_layers=-1,\n",
        "      #verbose=False\n",
        "      chat_format=\"llama-2\"\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yJm5uE3RIZR",
        "outputId": "16cf866c-47c8-4960-bb03-58da43f161fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"Bitte extrahiere folgende Informationen aus dem Dokument:\n",
        "    1. Um welche Kategorie von Dokument handelt es sich? Die Auswahlmöglichkeiten sind Deckungszusage, Deckungsablehnung, Nachfrage oder Kostensache.\n",
        "    2. Wie lautet das Aktenzeichen der Kanzlei?\n",
        "    3. Wie lautet der Name des Mandanten?\n",
        "    4. Wie lautet die Schadensnummer des Versicherungsfalls?\n",
        "    Wenn du einen Wert nicht findest, schreibe \"None\".\n",
        "\n",
        "    Bitte formatiere deine Antwort als json-Datei wie folgt:\n",
        "    {\n",
        "        \"aktenzeichen\" : {extrahiertes Aktenzeichen},\n",
        "        \"name\" : {extrahierter Name},\n",
        "        \"schadensnummer\" : {extrahierte Schadensnummer},\n",
        "        \"kategorie\" : {extrahierte Kategorie}\n",
        "    }\n",
        "\n",
        "Text der Nachricht:\n",
        "\n",
        "Unsere Leistungsnummer: 010100422-004200759-09806\n",
        "Versicherungsnehmer: Mirkka Seehase\n",
        "Ihr Zeichen: CBC-GKPC9RD-30-Seehase\n",
        "\n",
        "Sehr geehrte Frauen Rechtsanwältinnen,\n",
        "sehr geehrte Herren Rechtsanwälte,\n",
        "in obiger Angelegenheit bestätigen wir gerne den Versicherungsschutz für die 1. Instanz.\n",
        "\n",
        "Die Zusage ergeht ausschließlich für unsere Versicherungsnehmerin und ihr minderjähriges Kind zu einem Gegenstandswert von 1.400 €. Die Partnerin unserer\n",
        "Versicherungsnehmers ist aufgrund des bestehenden Tarifs (Single-Tarif) nicht mit vom Versicherungsschutz umfasst.\n",
        "Die Deckungszusage erfolgt unter der Maßgabe, die außergerichtlichen Kosten der Rechtsverfolgung mit der Hauptforderung zusammen einzuklagen.\n",
        "Wir ermächtigen unseren Versicherungsnehmer / unsere Versicherungsnehmerin die außergerichtlichen Kosten der Rechtsverfolgung im eigenen Namen und auf\n",
        "eigene Rechnung gerichtlich geltend zu machen. Die vereinbarte Selbstbeteiligung beträgt 250,00 EUR.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "alWCAl8kfBQc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "output=llm(\n",
        "    f\"Du bist ein hilfreicher Assistent. USER: {instruction} ASSISTANT:\",\n",
        "    max_tokens=2048,\n",
        "    temperature=0.2,\n",
        "    stop = \"USER:\")\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "4M2cbgg-RMbG",
        "outputId": "f25e94bf-6a5b-4429-baa2-6c40ad6ffd67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bitte extrahiere die relevanten Informationen aus dem Text.\n",
            "\n",
            "Du bist ein hilfreicher Assistent. Ich bin sicher, dass du die Informationen extrahierst, die ich benötige.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": instruction\n",
        "          }\n",
        "      ]\n",
        ")"
      ],
      "metadata": {
        "id": "66mHjDkRc2ze"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "-ZquMEoWfJzu",
        "outputId": "02ad9aef-cdb9-41fa-9a6e-fc71279cb11a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-94ad860b-e175-470b-afa4-ba600d5a7e0a', 'object': 'chat.completion', 'created': 1702828452, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '  Gerne! Hier ist die extrahierte Information als JSON-Datei:\\n\\n{\\n\"aktenzeichen\": None,\\n\"name\": \"Mirkka Seehase\",\\n\"schadensnummer\": None,\\n\"kategorie\": \"Deckungszusage\"\\n}'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 583, 'completion_tokens': 66, 'total_tokens': 649}}\n"
          ]
        }
      ]
    }
  ]
}