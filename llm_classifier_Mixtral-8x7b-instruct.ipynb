{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM6BR0oGL7vNaYKxaEn75s7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-mueller92/llm_mail_classifier/blob/main/llm_classifier_Mixtral-8x7b-instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KpilxyyZFEL",
        "outputId": "0c8f09b0-20d1-41f1-f236-cd638db9d3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.24)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzPEXpkLaiOz",
        "outputId": "03280b01-9c77-451b-e2fa-d25ee71c613a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "\n",
        "# dont do this at home\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Ha7gFhdjam4A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/dataset_anonymized.xlsx')"
      ],
      "metadata": {
        "id": "5D9bcKvBar0D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(df, idx, step=1):\n",
        "\n",
        "    file_content = df[\"txt\"][idx]\n",
        "\n",
        "    task1 = \"\"\"Bitte extrahiere folgende Informationen aus dem Dokument:\n",
        "    1. Um welche Kategorie von Dokument handelt es sich? Die Auswahlmöglichkeiten sind \"Deckungszusage\", \"Deckungsablehnung\", \"Nachfrage\" oder \"Kostensache\". Bei einer\n",
        "    Deckungszusage, erklärt die Versicherung, sie wird die zukünftigen Kosten des Falls übernehmen - bei einer Deckungszusage verneint sie das. Bei einer Nachfrage\n",
        "    möchte die Versicherung mehr Informationen von der Kanzlei. Eine Kostensache liegt nur vor, wenn die Versicherung erklärt, sie hat bereits Kosten bezahlt.\n",
        "    2. Wie lautet das Aktenzeichen der Kanzlei?\n",
        "    3. Wie lautet der Name des Mandanten?\n",
        "    4. Wie lautet die Schadensnummer des Versicherungsfalls?\n",
        "    Wenn du einen Wert nicht findest, schreibe \"null\".\n",
        "\n",
        "    Bitte formatiere deine Antwort als json-Datei wie folgt:\n",
        "    {\n",
        "        \"aktenzeichen\" : {extrahiertes Aktenzeichen},\n",
        "        \"name\" : {extrahierter Name},\n",
        "        \"schadensnummer\" : {extrahierte Schadensnummer},\n",
        "        \"kategorie\" : {extrahierte Kategorie: Deckungszusage - Deckungsablehnung - Nachfrage - Kosten}\n",
        "    }\"\"\"\n",
        "\n",
        "    task_dz = \"\"\"Bitte extrahiere folgende Informationen aus der Deckungszusage.\n",
        "    Auf welche Instanz bezieht sich die Deckungszusage? Zur Auswahl steht das außergerichtliche Verfahren, das gerichtliche Verfahren der ersten Instanz und das\n",
        "    Berufungsverfahren (2. Instanz).\n",
        "    Was ist der Betrag der Selbstbeteiligung der Deckungszusage? Wenn keine Selbstbeteiligung erwähnt ist, antworte mit 0.\n",
        "\n",
        "    Bitte formatiere deine Antwort als json-Datei wie folgt:\n",
        "    {\n",
        "        \"außergerichtlich\" : true/false,\n",
        "        \"gerichtlich\" : true/false,\n",
        "        \"berufung\" : true/false,\n",
        "        \"selbstbeteiligung\" : {extrahierter Betrag}\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    task_nachfrage = \"\"\"1. Welche Nachfragegründe kannst du in der Nachfrage finden? Fragt die Rechtsschutzversicherung nach dem Sachstand? Fragt Sie nach der Klage?\n",
        "    Fragt Sie nach dem Anspruchsschreiben? Erinnert Sie an ein vorheriges Schreiben? Fragt Sie nach sonstigen Fragen?\n",
        "\n",
        "    2. Bitte füge alle gefundenen Antworten zu einer Liste zusammen.\n",
        "\n",
        "    3. Bitte formatiere deine Antwort im json-Format wie folgt:\n",
        "    {\n",
        "        \"Nachfragen\" : [Liste der gefundenen Nachfragegründe: \"Sachstand\", \"Klage\", \"Anspruchschreiben\", \"Erinnerung\", \"Sonstiges\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    if step == 1:\n",
        "        prompt = file_content + \"-Ende der Nachricht-\\n\" + task1\n",
        "    elif step == 2 and df[\"Typ\"][idx] == \"Deckungszusage\":\n",
        "        prompt = file_content + \"-Ende der Deckungszusage-\\n\" + task_dz\n",
        "    elif step == 2 and df[\"Typ\"][idx] == \"Nachfrage\":\n",
        "        prompt = file_content + \"-Ende der Nachfrage-\\n\" + task_nachfrage\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def send_request(prompt):\n",
        "    response = llm.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt\n",
        "          }\n",
        "      ],\n",
        "      max_tokens=2048,\n",
        "      temperature=0.2,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "#convert response and save dict to df\n",
        "def save_response(response, idx, step):\n",
        "    column_name = f'response_{step}_{model}'\n",
        "    if column_name not in df.columns:\n",
        "        df[column_name] = None\n",
        "\n",
        "    # llama and mistral\n",
        "    if model == \"llama-13b\" or model == \"mistral-7b\" or model == \"mixtral-8x7b\":\n",
        "        try:\n",
        "            response_text = response['choices'][0]['message']['content'].strip()\n",
        "            reponse_json  = extract_json(response_text)\n",
        "\n",
        "            response_dict = json.loads(reponse_json)\n",
        "            df[column_name][idx] = response_dict\n",
        "\n",
        "            if len(response_dict) > 4:\n",
        "                print(\"Unexpected number of arguments in dict!\")\n",
        "            else:\n",
        "                for key, value in response_dict.items():\n",
        "                    if f'{key}_{model}' not in df.columns:\n",
        "                        df[f'{key}_{model}'] = ''\n",
        "                    df.at[idx, f'{key}_{model}'] = value\n",
        "\n",
        "        except:\n",
        "            df[column_name][idx] = response_text\n",
        "            print(\"Antwort konnte nicht zu json formatiert werden\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_json(string):\n",
        "    # Define a regular expression pattern to match JSON\n",
        "    pattern = r'\\{(.|\\n)*\\}'\n",
        "\n",
        "    # Find the JSON part using regex\n",
        "    json_match = re.search(pattern, string)\n",
        "\n",
        "    if json_match:\n",
        "        extracted_json = json_match.group()\n",
        "        #print(extracted_json)\n",
        "    else:\n",
        "        extracted_json = \"No JSON found\"\n",
        "        #print(\"No JSON part found in the string.\")\n",
        "\n",
        "    return extracted_json"
      ],
      "metadata": {
        "id": "9Guq9b54as6k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "0LxNXNO_as9K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = hf_hub_download(repo_id='TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF', filename=\"mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\")"
      ],
      "metadata": {
        "id": "KZ5eeRY0cZzL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del llm"
      ],
      "metadata": {
        "id": "6X1Yo3yphPWW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = Llama(\n",
        "  model_path = modelpath, # Download the model file first\n",
        "  n_ctx=1024,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "  n_gpu_layers=12,        # The number of layers to offload to GPU, if you have GPU acceleration available\n",
        "  chat_format = \"llama-2\"\n",
        "  #stop = \"<</SYS>>\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z62mGUiZas_W",
        "outputId": "47fb1024-02ff-4efa-ca1f-fb3c95ff8c70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Nachricht:\n",
        "\n",
        "Rechtsschutz-Schaden-Nr.: 2226-097.039.2-324\n",
        "Ihr Aktenzeichen: CBC-TE3HYKD-442-BRUNNER\n",
        "Unsere Kundin: Simone Brunner\n",
        "Ereignis vom: 26. August 2022\n",
        "\n",
        "Sehr geehrte Damen und Herren,\n",
        "für die Interessenwahrnehmung erster Instanz besteht Versicherungsschutz.\n",
        "Angesprochener Leistungsbereich ist der Rechtsschutz im Vertrags- und Sachenrecht.\n",
        "Kosten bei einer einverständlichen Erledigung trägt der Versicherer, soweit die Kostenverteilung dem Verhältnis des angestrebten zum erzielten Erfolg entspricht. Das gilt nur dann nicht, wenn eine hiervon abweichende Kostenregelung gesetzlich vorgeschrieben ist. Im Falle eines gerichtlichen Vergleiches achten Sie bitte auf eine deutliche Einbeziehung etwaiger\n",
        "Geschäftsgebühren. Etwaige (Mehr-) Kosten durch die Erledigung unstreitiger oder nicht versicherter Punkte sind nicht\n",
        "rechtsschutzversichert\n",
        "\n",
        "- Ende der Nachricht -\n",
        "\n",
        "Bitte extrahiere folgende Informationen aus der Nachricht:\n",
        "1. Um welche Kategorie von Dokument handelt es sich? Die Auswahlmöglichkeiten sind \"Deckungszusage\", \"Deckungsablehnung\", \"Nachfrage\" oder \"Kostensache\". Bei einer\n",
        "Deckungszusage, erklärt die Versicherung, sie wird die zukünftigen Kosten des Falls übernehmen - bei einer Deckungszusage verneint sie das. Bei einer Nachfrage\n",
        "möchte die Versicherung mehr Informationen von der Kanzlei. Eine Kostensache liegt nur vor, wenn die Versicherung erklärt, sie hat bereits Kosten bezahlt.\n",
        "2. Wie lautet das Aktenzeichen der Kanzlei?\n",
        "3. Wie lautet der Name des Mandanten?\n",
        "4. Wie lautet die Schadensnummer oder Leistungsnummer des Versicherungsfalls?\n",
        "Wenn du einen Wert nicht findest, schreibe \"None\".\n",
        "Bitte formatiere deine Antwort als json-Datei wie folgt:\n",
        "{\n",
        "\"aktenzeichen\" : {extrahiertes Aktenzeichen},\n",
        "\"name\" : {extrahierter Name},\n",
        "\"schadensnummer\" : {extrahierte Schadensnummer},\n",
        "\"kategorie\" : {extrahierte Kategorie: Deckungszusage - Deckungsablehnung - Nachfrage - Kosten (nur wenn bereits bezahlt)}\n",
        "}\"\"\""
      ],
      "metadata": {
        "id": "cy4zfu9Uc9hj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.create_chat_completion(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "4FdgtD2fatBl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3kTBkrmfWoY",
        "outputId": "63a75274-1d9e-4600-8d95-f9172ef7583b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-4475bdd2-c31c-4d80-acd4-02a978b2f714', 'object': 'chat.completion', 'created': 1702980894, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-Instruct-v0.1-GGUF/snapshots/fa1d3835c5d45a3a74c0b68805fcdc133dba2b6a/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' {\\n\"aktenzeichen\": \"CBC-TE3HYKD-442-BRUNNER\",\\n\"name\": \"Simone Brunner\",\\n\"schadensnummer\": \"2226-097.039.2-324\",\\n\"kategorie\": \"Deckungszusage\"\\n}'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 704, 'completion_tokens': 78, 'total_tokens': 782}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['choices'][0]['message']['content'].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KVDy65mfgVNe",
        "outputId": "fc5498ed-83c3-40eb-ce06-92669604701e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n\"aktenzeichen\": \"CBC-TE3HYKD-442-BRUNNER\",\\n\"name\": \"Simone Brunner\",\\n\"schadensnummer\": \"2226-097.039.2-324\",\\n\"kategorie\": \"Deckungszusage\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"mixtral-8x7b\"\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "\n",
        "    # first prompt/step\n",
        "    prompt = generate_prompt(df, idx, step=1)\n",
        "    response = send_request(prompt)\n",
        "\n",
        "    print(response['choices'][0]['message']['content'])\n",
        "\n",
        "    # write code to extract json only\n",
        "    # and add it here\n",
        "    save_response(response, idx, step=1)\n",
        "\n",
        "    # second prompt/step only for \"Deckungszusage\" and \"Nachfrage\"\n",
        "    if df[\"Typ\"][idx] == \"Deckungszusage\" or df[\"Typ\"][idx] == \"Nachfrage\":\n",
        "        prompt2 = generate_prompt(df, idx, step=2)\n",
        "        response2 = send_request(prompt2)\n",
        "        # check if json only was returned\n",
        "        save_response(response2, idx, step=2)\n",
        "\n",
        "    print(f\"Successfully processed entry {idx +1} of {df.shape[0]}\")\n",
        "\n",
        "    if idx >= 2:\n",
        "        break\n",
        "\n",
        "df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP2-b_IXeMx7",
        "outputId": "7b758bfb-b837-4946-a156-17a2a665d3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " {\n",
            "\"aktenzeichen\": \"None\",\n",
            "\"name\": \"Denver Pearson\",\n",
            "\"schadensnummer\": \"S-21-03384164\",\n",
            "\"kategorie\": \"Nachfrage\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    }
  ]
}